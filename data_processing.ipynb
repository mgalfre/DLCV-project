{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54a91e71-c2ba-4a21-a81d-cb0920512d9f",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b60333c-0247-4b58-9f8c-dd148fe050e6",
   "metadata": {},
   "source": [
    "With this notebook we perform all the necessary steps to prepare the images and annotations so that they are available to be used by our models. Please note that running this notebook takes a long time because the steps performed below include file download, preprocessing, moving and image transformations. We therefore suggest not to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59067fbb-f201-4372-9f8d-6a9325f4fc2f",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Downloading files](#download) <br>\n",
    "2. [Reading json files](#read) <br>\n",
    "3. [Removing unnecessary images](#remove) <br>\n",
    "4. [Train, Val, Test split](#ttv) <br>\n",
    "5. [Generating labels for YOLO](#generate) <br>\n",
    "6. [Generating grayscale dataset](#grayscale) <br>\n",
    "    6.1 [Three channels grayscale images](#three) <br>\n",
    "    6.2 [One channel grayscale images](#one) <br>\n",
    "7. [Colorize 1 channel grayscale image](#colorize) <br>\n",
    "8. [Solving class imbalance](#imbalance) <br>\n",
    "    8.1 [Undersampling](#under) <br>\n",
    "    8.2 [Oversampling](#over) <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a747051-ea65-435b-bec5-bbdd08a6e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import glob\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c55854-b022-4145-8bc6-539e9566f71f",
   "metadata": {},
   "source": [
    "## 1. Downloading files <a class=\"anchor\" id=\"download\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839eda1-fb26-4ff7-923f-d58fbc471ed0",
   "metadata": {},
   "source": [
    "Downloading the ```Aves.tar.gz``` zip file from [OneDrive](https://bocconi-my.sharepoint.com/:u:/g/personal/debora_nozza_unibocconi_it/EWj145j9O41NjVADGAGDJxoBe8QQkbogIY0aTw45YLKBmg?e=WbgCSa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a910c941-a259-4df6-9dd7-240e6eccdf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-12 08:40:55--  https://bocconi-my.sharepoint.com/:u:/g/personal/debora_nozza_unibocconi_it/EWj145j9O41NjVADGAGDJxoBe8QQkbogIY0aTw45YLKBmg?download=1\n",
      "Resolving bocconi-my.sharepoint.com... 52.105.130.25\n",
      "Connecting to bocconi-my.sharepoint.com|52.105.130.25|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /personal/debora_nozza_unibocconi_it/Documents/5.%20Corsi/Computer%20Vision%20-%202022-2023/DEEP%20LEARNING%20FOR%20COMPUTER%20VISION%20-%20PROJECT%20DATA/Aves.tar.gz?ga=1 [following]\n",
      "--2022-11-12 08:40:55--  https://bocconi-my.sharepoint.com/personal/debora_nozza_unibocconi_it/Documents/5.%20Corsi/Computer%20Vision%20-%202022-2023/DEEP%20LEARNING%20FOR%20COMPUTER%20VISION%20-%20PROJECT%20DATA/Aves.tar.gz?ga=1\n",
      "Reusing existing connection to bocconi-my.sharepoint.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 51068277745 (48G) [application/x-gzip]\n",
      "Saving to: ‘Aves.tar.gz’\n",
      "\n",
      "Aves.tar.gz         100%[===================>]  47.56G  50.1MB/s    in 13m 52s \n",
      "\n",
      "2022-11-12 08:54:48 (58.5 MB/s) - ‘Aves.tar.gz’ saved [51068277745/51068277745]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_url = 'https://bocconi-my.sharepoint.com/:u:/g/personal/debora_nozza_unibocconi_it/EWj145j9O41NjVADGAGDJxoBe8QQkbogIY0aTw45YLKBmg?e=WbgCSa'\n",
    "output_dir = \"Aves.tar.gz\"\n",
    "\n",
    "split_url = input_url.rfind('?')\n",
    "converted_url = input_url[:split_url] + '?download=1'\n",
    "\n",
    "!wget -O \"$output_dir\" \"$converted_url\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fe847-7625-44bd-bd9b-ba99246c6e1b",
   "metadata": {},
   "source": [
    "The downloaded zip weighs 47.6 GB. Unzipping it completely would mean uploading another 47.6 GB to the directory. Since we don't need all the categories contained in this file, we used the following code to enter the zipped file and unzip only the 10 categories of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6208457-8bca-47b7-b51b-6e5697995439",
   "metadata": {},
   "outputs": [],
   "source": [
    "aves = tarfile.open(\"Aves.tar.gz\") # opening the zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b926d0c2-23f4-4881-ab1d-42992c60d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following are the most populated categories, thus the ones we want to unzip\n",
    "new_cat = ['Ardea herodias',\n",
    " 'Buteo jamaicensis',\n",
    " 'Ardea alba',\n",
    " 'Melospiza melodia',\n",
    " 'Cardinalis cardinalis',\n",
    " 'Zenaida macroura',\n",
    " 'Agelaius phoeniceus',\n",
    " 'Pandion haliaetus',\n",
    " 'Junco hyemalis',\n",
    " 'Picoides pubescens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0fd8380-3608-4a6a-926c-e4ae023e2bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64f3206d6b449a5bd89e926ae3e556d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in tqdm(new_cat):\n",
    "    lista = [l for l,i in enumerate(aves.getnames()) if i.startswith(f\"Aves/{c}/\")]\n",
    "    #print(lista)\n",
    "    for l in lista:\n",
    "        aves.extract(aves.getmembers()[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ebab7-848f-48fd-bcab-06d5453206d6",
   "metadata": {},
   "source": [
    "## 2. Reading json files <a class=\"anchor\" id=\"read\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0779d8-5e11-46b2-9218-b73facf95f50",
   "metadata": {},
   "source": [
    "Images labels and bounding boxes information are stored in _.json files_. Next cells of code are importing these files and storing relevant info into pandas dataframe. These _.json files_ were downloaded from the [iNaturalist](https://github.com/visipedia/inat_comp/tree/master/2017) github page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "799d2d1a-6d59-4d40-abc0-bbd5dda0989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SATM.preprocess import merge_aves_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e71e84-d23d-41f5-8f5b-d4d790b546b6",
   "metadata": {},
   "source": [
    "We have decided not to maintain the Train Val split provided by iNaturalist for three reasons:\n",
    "1. on the iNaturalist page the subdivision into two sets (train and val) is incompatible with our requirements, because we need a subdivision into train dev and test.\n",
    "2. the split in train dev of iNauralist is excessively unbalanced towards the train and there are very few images allocated in the val dataset to validate the performance\n",
    "3. the third reason is that the val dataset proposed by iNaturalist is not exactly representative of the entire dataset, since only in the train there are images with more than one box, while in the val there are only images featuring one box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599a15bc-6fb2-4516-a836-8be505dc6c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>file_name</th>\n",
       "      <th>rights_holder</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>id</th>\n",
       "      <th>area</th>\n",
       "      <th>iscrowd</th>\n",
       "      <th>image_id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>category_id</th>\n",
       "      <th>id_y</th>\n",
       "      <th>identifier</th>\n",
       "      <th>category_name</th>\n",
       "      <th>super_category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139706</th>\n",
       "      <td>3</td>\n",
       "      <td>train_val_images/Aves/Bubulcus ibis/26a9157b48...</td>\n",
       "      <td>greglasley</td>\n",
       "      <td>545</td>\n",
       "      <td>800</td>\n",
       "      <td>213252</td>\n",
       "      <td>37238.0</td>\n",
       "      <td>0</td>\n",
       "      <td>213252</td>\n",
       "      <td>[230, 199, 433, 172]</td>\n",
       "      <td>2912</td>\n",
       "      <td>153173</td>\n",
       "      <td>26a9157b48f66f71032f75ac70a11db7.jpg</td>\n",
       "      <td>Bubulcus ibis</td>\n",
       "      <td>Aves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214110</th>\n",
       "      <td>3</td>\n",
       "      <td>train_val_images/Insecta/Feltia herilis/8e7ecc...</td>\n",
       "      <td>leplady0209</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>318689</td>\n",
       "      <td>53133.5</td>\n",
       "      <td>0</td>\n",
       "      <td>318689</td>\n",
       "      <td>[194, 138, 323, 329]</td>\n",
       "      <td>4169</td>\n",
       "      <td>235593</td>\n",
       "      <td>8e7ecc6f1bf06ad53acb8326b8696740.jpg</td>\n",
       "      <td>Feltia herilis</td>\n",
       "      <td>Insecta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429742</th>\n",
       "      <td>3</td>\n",
       "      <td>train_val_images/Aves/Tringa solitaria/a766afd...</td>\n",
       "      <td>J. Maughn</td>\n",
       "      <td>591</td>\n",
       "      <td>800</td>\n",
       "      <td>598727</td>\n",
       "      <td>6950.0</td>\n",
       "      <td>0</td>\n",
       "      <td>598727</td>\n",
       "      <td>[113, 274, 139, 100]</td>\n",
       "      <td>3890</td>\n",
       "      <td>468382</td>\n",
       "      <td>a766afd4faa3b87cece8760ebaa6d9d1.jpg</td>\n",
       "      <td>Tringa solitaria</td>\n",
       "      <td>Aves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191185</th>\n",
       "      <td>3</td>\n",
       "      <td>train_val_images/Insecta/Manduca sexta/4f86fdb...</td>\n",
       "      <td>hobiecat</td>\n",
       "      <td>732</td>\n",
       "      <td>800</td>\n",
       "      <td>284974</td>\n",
       "      <td>194892.0</td>\n",
       "      <td>0</td>\n",
       "      <td>284974</td>\n",
       "      <td>[146, 56, 654, 596]</td>\n",
       "      <td>3771</td>\n",
       "      <td>210301</td>\n",
       "      <td>4f86fdb62ca07b7257a1ab9a72816d98.jpg</td>\n",
       "      <td>Manduca sexta</td>\n",
       "      <td>Insecta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458018</th>\n",
       "      <td>3</td>\n",
       "      <td>train_val_images/Aves/Haemorhous mexicanus/e28...</td>\n",
       "      <td>cuskelly</td>\n",
       "      <td>600</td>\n",
       "      <td>800</td>\n",
       "      <td>631689</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>631689</td>\n",
       "      <td>[269, 129, 125, 352]</td>\n",
       "      <td>4506</td>\n",
       "      <td>498071</td>\n",
       "      <td>e28ff6dc89c379783c08bba85f66371c.jpg</td>\n",
       "      <td>Haemorhous mexicanus</td>\n",
       "      <td>Aves</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       license                                          file_name  \\\n",
       "139706       3  train_val_images/Aves/Bubulcus ibis/26a9157b48...   \n",
       "214110       3  train_val_images/Insecta/Feltia herilis/8e7ecc...   \n",
       "429742       3  train_val_images/Aves/Tringa solitaria/a766afd...   \n",
       "191185       3  train_val_images/Insecta/Manduca sexta/4f86fdb...   \n",
       "458018       3  train_val_images/Aves/Haemorhous mexicanus/e28...   \n",
       "\n",
       "       rights_holder  height  width      id      area  iscrowd  image_id  \\\n",
       "139706    greglasley     545    800  213252   37238.0        0    213252   \n",
       "214110   leplady0209     600    800  318689   53133.5        0    318689   \n",
       "429742     J. Maughn     591    800  598727    6950.0        0    598727   \n",
       "191185      hobiecat     732    800  284974  194892.0        0    284974   \n",
       "458018      cuskelly     600    800  631689   22000.0        0    631689   \n",
       "\n",
       "                        bbox  category_id    id_y  \\\n",
       "139706  [230, 199, 433, 172]         2912  153173   \n",
       "214110  [194, 138, 323, 329]         4169  235593   \n",
       "429742  [113, 274, 139, 100]         3890  468382   \n",
       "191185   [146, 56, 654, 596]         3771  210301   \n",
       "458018  [269, 129, 125, 352]         4506  498071   \n",
       "\n",
       "                                  identifier         category_name  \\\n",
       "139706  26a9157b48f66f71032f75ac70a11db7.jpg         Bubulcus ibis   \n",
       "214110  8e7ecc6f1bf06ad53acb8326b8696740.jpg        Feltia herilis   \n",
       "429742  a766afd4faa3b87cece8760ebaa6d9d1.jpg      Tringa solitaria   \n",
       "191185  4f86fdb62ca07b7257a1ab9a72816d98.jpg         Manduca sexta   \n",
       "458018  e28ff6dc89c379783c08bba85f66371c.jpg  Haemorhous mexicanus   \n",
       "\n",
       "       super_category_name  \n",
       "139706                Aves  \n",
       "214110             Insecta  \n",
       "429742                Aves  \n",
       "191185             Insecta  \n",
       "458018                Aves  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merge_aves_df(\"train_2017_bboxes.json\", \"val_2017_bboxes.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711790f6-2cfb-4d96-b27b-642a2cc64321",
   "metadata": {},
   "source": [
    "## 3. Removing unncessary images <a class=\"anchor\" id=\"remove\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f0acb-da5a-4ef6-9696-c79c40b74fbb",
   "metadata": {},
   "source": [
    "There are photos of birds in the dataset for which bounding box coordinates have not been provided. We don't need these images, because our goal is to do object detection. On the other side there are images in the df pandas dataframe, for which we don't have the corresponding _.jpg file_ (image) downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2533e82c-bc9a-47d8-9e75-84eb66548837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint removed\n",
      "checkpoint removed\n"
     ]
    }
   ],
   "source": [
    "from SATM.preprocess import clean_aves\n",
    "category_list = os.listdir(\"Aves\") # retrieving the list of categories\n",
    "clean_aves(cat_list = category_list, path = \"Aves\", element = \".ipynb_checkpoints\") # removing dirty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02804e47-b2ae-416a-8fce-765b8a3dda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SATM.preprocess import encode_df\n",
    "img_list = [i for cat in category_list for i in os.listdir(\"Aves/\"+cat)] # retrieving the list of images available\n",
    "df = df[df.identifier.isin(img_list)]\n",
    "df = encode_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b7e1ee-137e-4d20-8739-a5eb6c53e73a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01742a3d8bb452889991435bdda6d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 pictures to be removed\n"
     ]
    }
   ],
   "source": [
    "# there are images we downloaded that do not feature a bounding box. We need to delete them\n",
    "# to identify them, we look for pictures not appearing in the dataframe\n",
    "img_list = [i for cat in category_list for i in os.listdir(\"Aves/\"+cat)]\n",
    "identifiers = list(df.identifier) # list of pictures not appearing in the dataframe\n",
    "to_remove = [i for i in tqdm(img_list) if i not in identifiers] # retrieving the list of images to be deleted\n",
    "print(f'There are {len(to_remove)} pictures to be removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620b7ac2-7850-4913-a10e-5eb4d51f9195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pictures removed!\n"
     ]
    }
   ],
   "source": [
    "# deleting images\n",
    "path = \"Aves\"\n",
    "rem_count = 0\n",
    "for i in category_list:\n",
    "    temp_path = path+\"/\"+i\n",
    "    list_dir = os.listdir(temp_path)\n",
    "    for k in to_remove:\n",
    "        if k in list_dir:\n",
    "            os.remove(temp_path+\"/\"+k)\n",
    "            rem_count += 1\n",
    "print(f'{rem_count} pictures removed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f8addcc-af13-4cf5-8d17-7c5e2d13393d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 22257 to 22257 images\n",
      "The number of rows in the dataframe is the same as the number of available images\n"
     ]
    }
   ],
   "source": [
    "new_img_list = [i for cat in category_list for i in os.listdir(path+\"/\"+cat)]\n",
    "print(f'From {len(img_list)} to {len(new_img_list)} images')\n",
    "if len(new_img_list) == len(df.drop_duplicates(\"image_id\")):\n",
    "    print(\"The number of rows in the dataframe is the same as the number of available images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8cb76-eb56-4359-8071-212485670f6b",
   "metadata": {},
   "source": [
    "## 4. Train, Val, Test split <a class=\"anchor\" id=\"tvt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf865a7-9b3e-4ad2-87ab-029553cee380",
   "metadata": {},
   "source": [
    "As previously mentioned, we have decided to adopt a different split from the one proposed by iNaturalist. To make a random split, we shuffle the dataset containing the image information, and then we move the _.jpg files_ into the appropriate ```Train```, ```Val```, and ```Test``` folders. We decided to use 80%-10%-10% as the split percentage for the three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd8181c-8980-4c9f-89ac-960040d61a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 810\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "images_df = df.drop_duplicates(\"image_id\")\n",
    "val_identifiers = []\n",
    "test_identifiers = []\n",
    "for cat in category_list:\n",
    "    temp_list = list(images_df[images_df.category_name == cat].sample(frac = 0.2, random_state = 0).identifier)\n",
    "    l = len(temp_list)//2\n",
    "    val_identifiers += temp_list[:l]\n",
    "    test_identifiers += temp_list[l:]\n",
    "    #print(len(val_identifiers), len(test_identifiers))\n",
    "random.shuffle(val_identifiers)\n",
    "random.shuffle(test_identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0dd0e0-3d9f-4587-b957-61d6dd267017",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_test_identifiers = val_identifiers+test_identifiers\n",
    "val_df = df[df.identifier.isin(val_identifiers)]\n",
    "test_df = df[df.identifier.isin(test_identifiers)]\n",
    "train_df = df[~df.identifier.isin(val_test_identifiers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f735b8c6-9a03-4ecb-91cb-0c5f78171b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_identifiers = list(train_df.drop_duplicates(\"image_id\").identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410499fa-a26d-40d6-8af8-5f0af42ebbb9",
   "metadata": {},
   "source": [
    "To check that we have performed the split correctly, we check, for each category, that the number of images available has remained unchanged. We do this check for the images and for the boxes (note that some images have more than one box inside, so the number of boxes and images may not coincide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33338d05-46b4-4f88-8c37-a93e3fe8ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split done correctly!\n"
     ]
    }
   ],
   "source": [
    "if len(train_df) + len(val_df) + len(test_df) == len(df):\n",
    "    print(\"Split done correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24e5e8c7-297a-4155-a7d8-0507793a8933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Train</th>\n",
       "      <th>Val</th>\n",
       "      <th>Test</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Melospiza melodia</th>\n",
       "      <td>2098</td>\n",
       "      <td>1676</td>\n",
       "      <td>213</td>\n",
       "      <td>209</td>\n",
       "      <td>2098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardea alba</th>\n",
       "      <td>3640</td>\n",
       "      <td>2917</td>\n",
       "      <td>356</td>\n",
       "      <td>367</td>\n",
       "      <td>3640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandion haliaetus</th>\n",
       "      <td>1999</td>\n",
       "      <td>1588</td>\n",
       "      <td>201</td>\n",
       "      <td>210</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardinalis cardinalis</th>\n",
       "      <td>2207</td>\n",
       "      <td>1758</td>\n",
       "      <td>220</td>\n",
       "      <td>229</td>\n",
       "      <td>2207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zenaida macroura</th>\n",
       "      <td>2502</td>\n",
       "      <td>1997</td>\n",
       "      <td>250</td>\n",
       "      <td>255</td>\n",
       "      <td>2502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agelaius phoeniceus</th>\n",
       "      <td>2348</td>\n",
       "      <td>1856</td>\n",
       "      <td>237</td>\n",
       "      <td>255</td>\n",
       "      <td>2348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Junco hyemalis</th>\n",
       "      <td>1385</td>\n",
       "      <td>1113</td>\n",
       "      <td>134</td>\n",
       "      <td>138</td>\n",
       "      <td>1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardea herodias</th>\n",
       "      <td>4299</td>\n",
       "      <td>3445</td>\n",
       "      <td>422</td>\n",
       "      <td>432</td>\n",
       "      <td>4299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buteo jamaicensis</th>\n",
       "      <td>3612</td>\n",
       "      <td>2874</td>\n",
       "      <td>372</td>\n",
       "      <td>366</td>\n",
       "      <td>3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Picoides pubescens</th>\n",
       "      <td>1546</td>\n",
       "      <td>1240</td>\n",
       "      <td>154</td>\n",
       "      <td>152</td>\n",
       "      <td>1546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Original Train  Val Test   Sum\n",
       "Melospiza melodia         2098  1676  213  209  2098\n",
       "Ardea alba                3640  2917  356  367  3640\n",
       "Pandion haliaetus         1999  1588  201  210  1999\n",
       "Cardinalis cardinalis     2207  1758  220  229  2207\n",
       "Zenaida macroura          2502  1997  250  255  2502\n",
       "Agelaius phoeniceus       2348  1856  237  255  2348\n",
       "Junco hyemalis            1385  1113  134  138  1385\n",
       "Ardea herodias            4299  3445  422  432  4299\n",
       "Buteo jamaicensis         3612  2874  372  366  3612\n",
       "Picoides pubescens        1546  1240  154  152  1546"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"original\", \"train\",\"val\",\"test\", \"sum\")\n",
    "recap_split_box = pd.DataFrame(columns = [\"Original\", \"Train\", \"Val\", \"Test\", \"Sum\"])\n",
    "for cat in category_list:\n",
    "    values = [len(df[df.category_name == cat]),\n",
    "          len(train_df[train_df.category_name == cat]),\n",
    "          len(val_df[val_df.category_name == cat]),\n",
    "            len(test_df[test_df.category_name == cat]),\n",
    "          sum([len(train_df[train_df.category_name == cat]),\n",
    "          len(val_df[val_df.category_name == cat]),\n",
    "          len(test_df[test_df.category_name == cat])])]\n",
    "    \n",
    "    recap_split_box.loc[cat,:] = values\n",
    "    \n",
    "recap_split_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa9f227e-b8a0-4baa-9be7-0589bb33fb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Train</th>\n",
       "      <th>Val</th>\n",
       "      <th>Test</th>\n",
       "      <th>Sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Melospiza melodia</th>\n",
       "      <td>2050</td>\n",
       "      <td>1640</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardea alba</th>\n",
       "      <td>2848</td>\n",
       "      <td>2278</td>\n",
       "      <td>285</td>\n",
       "      <td>285</td>\n",
       "      <td>2848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandion haliaetus</th>\n",
       "      <td>1794</td>\n",
       "      <td>1435</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardinalis cardinalis</th>\n",
       "      <td>2006</td>\n",
       "      <td>1605</td>\n",
       "      <td>200</td>\n",
       "      <td>201</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zenaida macroura</th>\n",
       "      <td>1932</td>\n",
       "      <td>1546</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>1932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Agelaius phoeniceus</th>\n",
       "      <td>1888</td>\n",
       "      <td>1510</td>\n",
       "      <td>189</td>\n",
       "      <td>189</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Junco hyemalis</th>\n",
       "      <td>1281</td>\n",
       "      <td>1025</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>1281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ardea herodias</th>\n",
       "      <td>3627</td>\n",
       "      <td>2902</td>\n",
       "      <td>362</td>\n",
       "      <td>363</td>\n",
       "      <td>3627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Buteo jamaicensis</th>\n",
       "      <td>3328</td>\n",
       "      <td>2662</td>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "      <td>3328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Picoides pubescens</th>\n",
       "      <td>1503</td>\n",
       "      <td>1202</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>1503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Original Train  Val Test   Sum\n",
       "Melospiza melodia         2050  1640  205  205  2050\n",
       "Ardea alba                2848  2278  285  285  2848\n",
       "Pandion haliaetus         1794  1435  179  180  1794\n",
       "Cardinalis cardinalis     2006  1605  200  201  2006\n",
       "Zenaida macroura          1932  1546  193  193  1932\n",
       "Agelaius phoeniceus       1888  1510  189  189  1888\n",
       "Junco hyemalis            1281  1025  128  128  1281\n",
       "Ardea herodias            3627  2902  362  363  3627\n",
       "Buteo jamaicensis         3328  2662  333  333  3328\n",
       "Picoides pubescens        1503  1202  150  151  1503"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"original\", \"train\",\"val\",\"test\", \"sum\")\n",
    "recap_split_image = pd.DataFrame(columns = [\"Original\", \"Train\", \"Val\", \"Test\", \"Sum\"])\n",
    "for cat in category_list:\n",
    "    values = [len(df.drop_duplicates(\"image_id\")[df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "          len(train_df.drop_duplicates(\"image_id\")[train_df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "          len(val_df.drop_duplicates(\"image_id\")[val_df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "          len(test_df.drop_duplicates(\"image_id\")[test_df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "         sum([len(train_df.drop_duplicates(\"image_id\")[train_df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "          len(val_df.drop_duplicates(\"image_id\")[val_df.drop_duplicates(\"image_id\").category_name == cat]),\n",
    "          len(test_df.drop_duplicates(\"image_id\")[test_df.drop_duplicates(\"image_id\").category_name == cat])])]\n",
    "    recap_split_image.loc[cat,:] = values\n",
    "    \n",
    "recap_split_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7bd63b1-c44d-47a1-918b-2a17d1310dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving dataframes\n",
    "with open('pickles/train.pickle', 'wb') as handle:\n",
    "    pickle.dump(train_df, handle)\n",
    "with open('pickles/val.pickle', 'wb') as handle:\n",
    "    pickle.dump(val_df, handle)\n",
    "with open('pickles/test.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_df, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e0b09e-16e9-47f5-999e-8555e2690ee9",
   "metadata": {},
   "source": [
    "Now that the split has been done at the dataframe level, let's perform the split in the files and directories. A new directory is used, structured in the following way:\n",
    "```\n",
    "data\n",
    "└───images\n",
    "│   └───Train\n",
    "│   │   └───img1.jpg\n",
    "│   │       img2.jpg\n",
    "│   │       ...\n",
    "│   └───Val\n",
    "│   │   └───img3.jpg\n",
    "│   │       img4.jpg\n",
    "│   │       ...\n",
    "│   └───Test\n",
    "│       └───img5.jpg\n",
    "│           img6.jpg\n",
    "│           ...\n",
    "│   \n",
    "└───labels\n",
    "    └───Train\n",
    "    │   └───txt1.txt\n",
    "    │       txt2.txt\n",
    "    │       ...\n",
    "    └───Val\n",
    "    │   └───txt3.txt\n",
    "    │       txt4.txt\n",
    "    │       ...\n",
    "    └───Test\n",
    "        └───txt5.txt\n",
    "            txt6.txt\n",
    "            ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc20f1a-8320-4484-ad2b-8956e86c14d5",
   "metadata": {},
   "source": [
    "This repository structure will be used both by Faster R-CNN and YOLO. The _.txt files_, created in [section 5](#generate) contains the labels (category id + bounding box) as required by YOLO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60502215-ad01-4c82-b02b-9a8f3f00c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copying the images into correct Train, Val, Test\n",
    "# accordingly with the train_df, val_df and test_df\n",
    "count_train = 0\n",
    "count_val = 0\n",
    "count_test = 0\n",
    "\n",
    "errors = [] # preparing a list to store any foto that is not correctly transferred\n",
    "\n",
    "\n",
    "for cat in tqdm(category_list): # iterating through the categories\n",
    "    temp_path = \"Aves/\"+cat\n",
    "    #print(temp_path)\n",
    "    temp_img_list = os.listdir(temp_path)\n",
    "    for i in tqdm(temp_img_list):\n",
    "        if i in train_identifiers:\n",
    "            try:\n",
    "                shutil.copyfile(temp_path+\"/\"+i, \"data/images/Train/\"+i)\n",
    "                count_train += 1\n",
    "            except:\n",
    "                errors.append(i)\n",
    "                print(id, \"train error\")\n",
    "\n",
    "        elif i in val_identifiers:\n",
    "            try:\n",
    "                shutil.copyfile(temp_path+\"/\"+i, \"data/images/Val/\"+i)\n",
    "                count_val += 1\n",
    "            except:\n",
    "                errors.append(i)\n",
    "                print(id, \"val error\")\n",
    "                \n",
    "                \n",
    "        elif i in test_identifiers:\n",
    "            try:\n",
    "                shutil.copyfile(temp_path+\"/\"+i, \"data/images/Test/\"+i)\n",
    "                count_test += 1\n",
    "            except:\n",
    "                errors.append(i)\n",
    "                print(id, \"test error\")\n",
    "                \n",
    "        else:\n",
    "            errors.append(i)\n",
    "            print(id, \"the image is not contained in the dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffde87-4631-4b90-9434-3894073ec322",
   "metadata": {},
   "source": [
    "## 5. Generating labels for YOLO <a class=\"anchor\" id=\"generate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b4668-1a6f-45f2-bcca-45ec62787726",
   "metadata": {},
   "source": [
    "YOLO needs data formatted in a particular way and that differs from the way in which they are provided by iNaturalist. To solve this incompatibility, we use the ```convert_to_yolo``` function, which prepares the data so that it can also be processed by YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e7ba635-3780-4f0b-b08d-be3a39fb3e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SATM.preprocess import convert_to_yolov5\n",
    "convert_to_yolov5(which = \"Train\", df = train_df)\n",
    "convert_to_yolov5(which = \"Val\", df = val_df)\n",
    "convert_to_yolov5(which = \"Test\", df = test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b6f4f90-f8b7-4f42-9211-49bd5e6eaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SATM.preprocess import clean_data\n",
    "clean_data() # clearing data from dirty files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e73f8-e91c-4d0e-964a-8ec9c97b2c14",
   "metadata": {},
   "source": [
    "## 6. Generating the grayscale dataset <a class=\"anchor\" id=\"grayscale\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84c6ed-a0db-44dc-b4a5-7ba139b622fc",
   "metadata": {},
   "source": [
    "Since we want to see the performance of the models on grayscale images (train on gray & test on gray, train on colored & test on gray, train on gray & test on colored), we need to convert the images to gray scale. To do this, it is sufficient to calculate the average across the three channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d963a-1d22-422c-a43e-fdcb95cb6ead",
   "metadata": {},
   "source": [
    "### 6.1 Three channels grayscale images <a class=\"anchor\" id=\"three\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2527939-5457-4bde-aebf-9fd89e5ab670",
   "metadata": {},
   "source": [
    "With the next code cell we generate the grayscale images, keeping 3 channels, so that they have the same format as the color images. New images are copied in the ```data_bw``` repository, structured as ```data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cfcf4798-e111-4925-9881-6221938ac5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2f3d59979f498f9c42a118e125cc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c80acb5cbe6470498d58170d6f037e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17806 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 532, 800])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a766f4078d4ef19fe00edbb70c21ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 800, 800])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23314cf30da489faa68d9bd453a69ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 800, 713])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "for folder in tqdm([\"Train\", \"Val\", \"Test\"]):\n",
    "    source_p = os.listdir(\"data/images/\"+folder)\n",
    "    for i in tqdm(source_p):\n",
    "        image_file = Image.open(f\"data/images/{folder}/{i}\") # open colour image\n",
    "        tensor = transform(image_file)\n",
    "        avg_tensor = tensor.mean(axis = 0).numpy()\n",
    "        new_image = torch.tensor(np.array([avg_tensor,avg_tensor,avg_tensor]))\n",
    "        print(new_image.shape)\n",
    "        save_image(new_image, f'data_bw/images/{folder}/{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26293a13-5065-44ee-b8cf-d37630130cb1",
   "metadata": {},
   "source": [
    "### 6.2 One channel grayscale images <a class=\"anchor\" id=\"one\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f8882-c659-433e-a7ad-7e646c8c33c9",
   "metadata": {},
   "source": [
    "With the next code cell, however, we generate the single-channel grayscale images. In this case, to become compatible with normal colored images, these monochannel grayscales must be recolored. New images are copied in the ```data_1channel``` repository, structured as ```data```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c08bb93-0038-4893-bd1f-dfceac8b9aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b650e85776841c3999e7e8d9cdbb3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d3a9d931b94d6ca21e1e1e9cfeeacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17805 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97b812609b94ba4bf60698a8bc762f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9361d44da674ba383253af79ccca864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "for folder in tqdm([\"Train\", \"Val\", \"Test\"]):\n",
    "    source_p = os.listdir(\"data/images/\"+folder)\n",
    "    for i in tqdm(source_p):\n",
    "        image_file = Image.open(f\"data/images/{folder}/{i}\") # open colour image\n",
    "        tensor = transform(image_file)\n",
    "        avg_tensor = tensor.mean(axis = 0)\n",
    "        #new_image = torch.tensor(np.array([avg_tensor,avg_tensor,avg_tensor]))\n",
    "        #print(new_image.shape)\n",
    "        save_image(avg_tensor, f'data_1channel/images/{folder}/{i}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935e3ba-9866-4e32-b3f7-875c856edad8",
   "metadata": {},
   "source": [
    "## 7. Colorize 1 channel grayscale images <a class=\"anchor\" id=\"colorize\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a4e18f-b7a8-476e-9769-73c4b81e87f7",
   "metadata": {},
   "source": [
    "We want to color grayscale single-channel images to be color and three-channel images to be processed by our models driven by color images. To do this we used the [DeOldify Image Colorization](https://deepai.org/machine-learning-model/colorizer) by DeepAI. It is a model based on GANs, capable of recoloring images and videos ([see GitHub here](https://github.com/jantic/DeOldify))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d936ba-ce9f-4e45-8a0d-2fb40fdd5016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/jantic/DeOldify.git DeOldify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a3b3681-77b0-4ab4-aed2-255265fe305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/Project/DeOldify\n"
     ]
    }
   ],
   "source": [
    "cd DeOldify/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddf48142-6799-4b6f-a74e-a39b73aa92e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'DeOldify # to run the colorizer we need to temporarily change the working directory'\n",
      "/home/labuser/Project\n"
     ]
    }
   ],
   "source": [
    "cd DeOldify/ # to run the colorizer we need to temporarily change the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aa8738c-2e0c-4cac-ac00-43fa6db30b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE:  This must be the first call in order to work properly!\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "#choices:  CPU, GPU0...GPU7\n",
    "device.set(device=DeviceId.GPU0)\n",
    "\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print('GPU not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c796b3f7-465c-4a09-b045-27228f43314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements-colab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "551c08ff-b8ed-4929-8f48-652b6cf8668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from deoldify.visualize import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*?Your .*? set is empty.*?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c70cec92-c96d-4fa5-8360-b0f8126daf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-02 18:19:11--  https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth\n",
      "Resolving data.deepai.org... 5.9.140.253\n",
      "Connecting to data.deepai.org|5.9.140.253|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 255144681 (243M) [application/octet-stream]\n",
      "Saving to: ‘./models/ColorizeArtistic_gen.pth’\n",
      "\n",
      "./models/ColorizeAr 100%[===================>] 243.32M   108MB/s    in 2.3s    \n",
      "\n",
      "2022-12-02 18:19:13 (108 MB/s) - ‘./models/ColorizeArtistic_gen.pth’ saved [255144681/255144681]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir 'models'\n",
    "!wget https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth -O ./models/ColorizeArtistic_gen.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c9b81cc-f6a2-4e2e-8180-94040b4d2f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/anaconda/envs/py38_default/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "colorizer = get_image_colorizer(artistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62158c5b-19fe-48c9-b269-5663f1585171",
   "metadata": {},
   "source": [
    "Newly colored images are moved in the ```colorized``` folder, that is structured as ```data```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02647f92-b39d-4560-b22f-4ef09d6f8d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2415f3024e894301adf2b7abe55c528d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2224 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img in tqdm(os.listdir(\"../data/images/Val/\")):\n",
    "    img_pil = colorizer.get_transformed_image(\"../data/images/Val/\"+img,\n",
    "                                render_factor = 35)\n",
    "    img_pil.save(\"../colorized/images/Val/\"+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fb3b245-3da7-49fd-b32c-7307a3dab654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/labuser/Project\n"
     ]
    }
   ],
   "source": [
    "cd .. # setting back the main directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548a981-d7f3-4c1e-a5c5-86896257c0a0",
   "metadata": {},
   "source": [
    "## 8. Solving class imbalance <a class=\"anchor\" id=\"imbalance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50e481-6934-40a6-bcca-63d990bd2659",
   "metadata": {},
   "source": [
    "As shown in the ```data_visualization.ipynb``` notebook, the classes are not well balanced with each other. To solve this potential problem, we can use two techniques, that can be performed on the training dataset: _undersampling_ and _oversampling_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac7f154-bfec-452e-9ece-9db5f08c0911",
   "metadata": {},
   "source": [
    "![under-over](https://pulplearning.altervista.org/wp-content/uploads/2020/08/undersampling-oversampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0981dd-c691-4d1a-a369-ef32803511eb",
   "metadata": {},
   "source": [
    "Within the project, we will verify the performance of Faster R-CNN and Yolo in under and oversampling conditions.\n",
    "* For **Faster R-CNN** we used the ```SATMsampler``` class that we defined in the ```dataset.py``` file and then used in ```Faster_training.ipynb```\n",
    "* For **YOLO** instead it was necessary to create special directories, which contain the files consistently with the setting of interest (under and oversampling). This processing is shown in the code cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d3eb49-c75e-49e7-ab8b-9808f23a7e57",
   "metadata": {},
   "source": [
    "### 8.1 Undersampling <a class=\"anchor\" id=\"under\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68027b-074f-4e10-8888-f8d4248182d8",
   "metadata": {},
   "source": [
    "Undersampling simply consists of identifying the least represented class and randomly removing images from all other classes so that their number is reduced to the level of the underpopulated class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fcff71-8882-40c3-bd14-23b30033c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the number of images in the underpopulated class\n",
    "min_threshold = train_df.drop_duplicates(\"image_id\").groupby(\"category_name\").size().sort_values().head(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e6432f2d-a2f8-4b0c-9d62-b63b3d606ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40585650fbe445eda4983680382ff91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0f7491f6c74491b748c731d55af5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc38309d02845d1beab7b3a862faa3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56dbda1d0c044ccc89636b41e592125d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec06c0c1e5642c18c2c6fa2c852811a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968e00761b464ebd8f586b0147c1623f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de9d2d921404002a60c520029000e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5909575ae9514988a8030f04aa68161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d93a1040f9b4041bda350a2b6d0d406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcd4c8b079b4494b33ce9aa3ed8ce99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dead3c9ac5eb4c0381684ed59cd5c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(810) # setting a seed for reproducibility\n",
    "for cat in tqdm(category_list): # iterating through categories\n",
    "    temp_df = train_df.drop_duplicates(\"image_id\") # images-only train dataset\n",
    "    identifiers = temp_df[temp_df.category_name == cat].identifier.values # names of the images\n",
    "    chosen = np.random.choice(identifiers, min_threshold) # choosing random images to be dropped\n",
    "    for pic in tqdm(chosen): # removing images and txt files corresponding to the sampled images\n",
    "        shutil.copyfile(\"data/images/Train/\"+pic, \"data_under/images/Train/\"+pic)\n",
    "        shutil.copyfile(\"data/labels/Train/\"+pic[:-3]+\"txt\", \"data_under/labels/Train/\"+pic[:-3]+\"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e1721-cb42-43e2-97a6-21b0add548de",
   "metadata": {},
   "source": [
    "### 8.2 Oversampling <a class=\"anchor\" id=\"over\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156d7e8-8084-4e5d-b70e-0a993153077a",
   "metadata": {},
   "source": [
    "With oversampling instead we identify the most populated class and we bring to this level the number of images belonging to the other classes. This process is slightly more complicated, since to increase the underrepresented classes it is necessary to generate new images. To do this we use [Albumentation](https://github.com/albumentations-team/albumentations), an algorithm that allows us to transform images (in our case we apply rotations and flips), extending the transformation also to the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07b1ac81-6abd-471d-8454-284c6dd7cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying the number of images belonging to the most populated class\n",
    "max_threshold = train_df.drop_duplicates(\"image_id\").groupby(\"category_name\").size().sort_values().tail(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc611787-a38c-459f-a4a0-276d877a7058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many images need to be added per each category?\n",
    "to_add = max_threshold - train_df.drop_duplicates(\"image_id\").groupby(\"category_name\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb781294-3571-4ae1-b4d9-2635d0c1c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category_name\n",
       "Agelaius phoeniceus      1392\n",
       "Ardea alba                624\n",
       "Ardea herodias              0\n",
       "Buteo jamaicensis         240\n",
       "Cardinalis cardinalis    1297\n",
       "Junco hyemalis           1877\n",
       "Melospiza melodia        1262\n",
       "Pandion haliaetus        1467\n",
       "Picoides pubescens       1700\n",
       "Zenaida macroura         1356\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ff4b65-9ad9-4604-a7b8-5658d244288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing custom functions that write txt files with the corret label(s) and bounding box(es)\n",
    "from SATM.preprocess import generate_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3428c7f9-599f-4a1f-8123-039139bfff44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7586d07e6a4545a8710a0d7ed62243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6e18df2d5de418a9d46d18bb57690ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a452739c360648709ab764da5603a48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/624 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a18dbe073745a389969b1958eb0e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee91088c8514b36ad4d89991bf8accb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7992196b6ff9493981ae2f9a2584c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1297 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48cda634c35542c9a8619e71b8ffe37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1356 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743bcfb76c1e4e91b8e5ad37f0b6fa4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370c2d4ba91a49fcba2e4072f3bb5fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bd169d6b82439093b10d27c557fcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/852 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cad291504e487dbd01c4173d687eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "898ca9eaf87a4f57b7856b2e043a9c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0827a0ea03164bd8811e0d94a5a3c6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8d3b152b1a4c828509773f99bb5cdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(810) # setting a seed for reprudicibility\n",
    "\n",
    "# defining the two possibile transformations\n",
    "bbox_transform_flip = albumentations.Compose([albumentations.HorizontalFlip(p=1)],\n",
    "                                                    bbox_params = albumentations.BboxParams(format='pascal_voc',\n",
    "                                                                                            label_fields=['labels']))\n",
    "\n",
    "bbox_transform_rotate = albumentations.Compose([albumentations.Rotate(p=1)],\n",
    "                                                    bbox_params = albumentations.BboxParams(format='pascal_voc',\n",
    "                                                                                           label_fields=['labels']))\n",
    "wrong = [] # list for possible errors\n",
    "trans_dic = {}\n",
    "\n",
    "for cat in tqdm(category_list): # iterating across categories\n",
    "    #print(to_add[cat])\n",
    "    temp_df = train_df.drop_duplicates(\"image_id\") # images-only dataframe\n",
    "    identifiers = temp_df[temp_df.category_name == cat].identifier.values # list of images name\n",
    "    pota  = to_add[cat] # number of images to be added in the specific class\n",
    "    trans_dic[cat] = []\n",
    "    #print(len(identifiers))\n",
    "    \n",
    "    # there are two possible cases:\n",
    "        # 1. the current number of available images is enough to reach the upper threshold number\n",
    "            # in this case one (or less) than trasformation per image is sufficient.\n",
    "        # 2. instead, if the current number of images is not enough (that is, if transform once each\n",
    "            # image, we are not able to reach the threshold), more than one trasformation is needed.\n",
    "    \n",
    "    \n",
    "    # ------ CASE 1 -------\n",
    "    if len(identifiers) > pota: \n",
    "        to_transform = np.random.choice(identifiers, pota)\n",
    "        for img in tqdm(to_transform):\n",
    "            image = cv2.imread(\"data/images/Train/\"+img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            targets = generate_target(train_df[train_df.identifier == img])\n",
    "            try: # try to apply the transformation\n",
    "                transformed = bbox_transform_flip(image = image,\n",
    "                                             bboxes = targets['boxes'],\n",
    "                                             labels = targets['labels'])\n",
    "\n",
    "                trans_dic[cat].append(transformed)\n",
    "                generate_txt(img, transformed, \"_v1\")\n",
    "                save_image(torch.tensor(transformed[\"image\"]).cpu().permute(2,0,1)/255, \"data_over/images/Train/\"+img[:-4]+\"_v1\"+\".jpg\")\n",
    "            except:\n",
    "                wrong.append(img)\n",
    "\n",
    "            \n",
    "    # ------ CASE 2 -------       \n",
    "    else: \n",
    "        difference = pota - len(identifiers) # available images to be transformed\n",
    "        for img in tqdm(identifiers):\n",
    "            image = cv2.imread(\"data/images/Train/\"+img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            targets = generate_target(train_df[train_df.identifier == img])\n",
    "            \n",
    "            try: # try to apply the transormation\n",
    "                transformed = bbox_transform_flip(image = image,\n",
    "                                             bboxes = targets['boxes'],\n",
    "                                             labels = targets['labels'])\n",
    "\n",
    "                trans_dic[cat].append(transformed)\n",
    "                generate_txt(img, transformed, \"_v1\")\n",
    "                save_image(torch.tensor(transformed[\"image\"]).cpu().permute(2,0,1)/255, \"data_over/images/Train/\"+img[:-4]+\"_v1\"+\".jpg\")\n",
    "            except:\n",
    "                \n",
    "                wrong.append(img)\n",
    "                \n",
    "        to_transform = np.random.choice(identifiers, difference)\n",
    "        # re sampling from the previosuly transformed images and applying a different transformation\n",
    "        for img in tqdm(to_transform):\n",
    "            image = cv2.imread(\"data/images/Train/\"+img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            targets = generate_target(train_df[train_df.identifier == img])\n",
    "            try: # try to apply the transormation\n",
    "                \n",
    "                transformed = bbox_transform_rotate(image = image,\n",
    "                                         bboxes = targets['boxes'],\n",
    "                                         labels = targets['labels'])\n",
    "            \n",
    "            \n",
    "                trans_dic[cat].append(transformed)\n",
    "                generate_txt(img, transformed, \"_v2\")\n",
    "                save_image(torch.tensor(transformed[\"image\"]).cpu().permute(2,0,1)/255, \"data_over/images/Train/\"+img[:-4]+\"_v2\"+\".jpg\")\n",
    "            except:\n",
    "                wrong.append(img)                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
